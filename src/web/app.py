import sys
import os
import gradio as gr
import json
import traceback
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.core.detector import FakeNewsDetector
from config import WEB_CONFIG, GOOGLE_SEARCH_CONFIG, THRESHOLDS
from src.rules.detection_rules import DETECTION_RULES, KNOWLEDGE_CUTOFF_DATE
from src.utils.web_search import WebSearchValidator

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

def create_app(enable_web_search=False):
    """åˆ›å»ºGradio Webåº”ç”¨"""

    detector = FakeNewsDetector(
        enable_web_search=False,  # åˆå§‹ä¸å¯ç”¨ç½‘ç»œæœç´¢
        google_api_key=GOOGLE_SEARCH_CONFIG["API_KEY"],
        search_engine_id=GOOGLE_SEARCH_CONFIG["SEARCH_ENGINE_ID"]
    )
    
    # åˆ›å»ºwebæœç´¢éªŒè¯å™¨å®žä¾‹ï¼Œç”¨äºŽå•ç‹¬è¿›è¡Œç½‘ç»œæœç´¢
    web_validator = None
    if enable_web_search:
        web_validator = WebSearchValidator(
            api_key=GOOGLE_SEARCH_CONFIG["API_KEY"],
            search_engine_id=GOOGLE_SEARCH_CONFIG["SEARCH_ENGINE_ID"]
        )

    # å­˜å‚¨å½“å‰åˆ†æžç»“æžœçš„å˜é‡
    last_analysis_result = None
    last_processed_data = None

    def detect_news(title, content, url="", use_web_search=False):
        """æ‰§è¡Œæ–°é—»æ£€æµ‹
        
        Args:
            title: æ–°é—»æ ‡é¢˜
            content: æ–°é—»å†…å®¹
            url: æ–°é—»æ¥æºURL
            use_web_search: æ˜¯å¦ä½¿ç”¨ç½‘ç»œæœç´¢éªŒè¯
            
        Returns:
            è§„åˆ™åˆ†æžç»“æžœå’Œç»¼åˆç»“è®º
        """
        nonlocal last_analysis_result, last_processed_data
        
        try:
            # æž„é€ æ–°é—»æ•°æ®
            news_data = {
                "title": title,
                "content": content
            }
            
            if url:
                news_data["url"] = url
            
            # é¢„å¤„ç†æ•°æ®
            processed_data = detector.preprocess_news(news_data)
            
            # å¦‚æžœæ˜¯ç¬¬ä¸€æ¬¡åˆ†æžæˆ–è€…æ²¡æœ‰ç¼“å­˜ç»“æžœï¼Œæ‰§è¡Œå®Œæ•´åˆ†æž
            if not use_web_search or last_analysis_result is None:
                logger.info("æ‰§è¡Œå®Œæ•´æ–°é—»æ£€æµ‹")
                
                # æ‰§è¡Œæ£€æµ‹ï¼ˆä¸ä½¿ç”¨ç½‘ç»œæœç´¢ï¼‰
                result = detector.detect(news_data)
                
                # ä¿å­˜ç»“æžœå’Œå¤„ç†è¿‡çš„æ•°æ®ä»¥å¤‡åŽç”¨
                last_analysis_result = result
                last_processed_data = processed_data
            else:
                # å¦‚æžœæ˜¯ç½‘ç»œæœç´¢éªŒè¯ï¼Œä½¿ç”¨ç¼“å­˜çš„ç»“æžœ
                logger.info("ä½¿ç”¨ç¼“å­˜ç»“æžœï¼Œæ·»åŠ ç½‘ç»œæœç´¢éªŒè¯")
                result = last_analysis_result.copy()
                
                # å•ç‹¬æ‰§è¡Œç½‘ç»œæœç´¢éªŒè¯å¹¶æ·»åŠ åˆ°ç»“æžœä¸­
                if web_validator and enable_web_search:
                    # æ‰§è¡Œç½‘ç»œéªŒè¯ï¼Œç¡®ä¿ä½¿ç”¨æœ€å¤§ç»“æžœæ•°
                    web_result = web_validator.validate_news(processed_data, max_results=8)
                    
                    # è®°å½•ç»“æžœ
                    logger.info(f"ç½‘ç»œæœç´¢ç»“æžœï¼šæ‰¾åˆ°{len(web_result['validation_results'].get('sources', []))}æ¡ç»“æžœ")
                    
                    if web_result:
                        # æ·»åŠ ç½‘ç»œéªŒè¯ç»“æžœ
                        result["web_validation"] = web_result
                        
                        # è°ƒæ•´é£Žé™©è¯„åˆ†
                        risk_percentage = result["conclusion"]["risk_percentage"]
                        risk_adjustment = web_result["risk_adjustment"]
                        adjusted_risk = max(0, min(100, risk_percentage + risk_adjustment))
                        
                        # æ›´æ–°é£Žé™©è¯„ä¼°
                        result["conclusion"]["risk_percentage"] = adjusted_risk
                        result["conclusion"]["explanation"] += f"\n\nç½‘ç»œéªŒè¯è°ƒæ•´: {web_result['explanation']}"
                        
                        # é‡æ–°è®¡ç®—é£Žé™©ç­‰çº§
                        if adjusted_risk >= THRESHOLDS["HIGH_RISK_THRESHOLD"]:
                            result["risk_level"] = "é«˜é£Žé™©"
                        elif adjusted_risk <= THRESHOLDS["LOW_RISK_THRESHOLD"]:
                            result["risk_level"] = "ä½Žé£Žé™©"
                        else:
                            result["risk_level"] = "ä¸­ç­‰é£Žé™©"
                    else:
                        logger.warning("ç½‘ç»œæœç´¢æœªè¿”å›žæœ‰æ•ˆç»“æžœ")
            
            # æå–è§„åˆ™åˆ†æžç»“æžœ
            rules_output = "## è§„åˆ™åˆ†æž\n\n"
            
            for i, rule in enumerate(DETECTION_RULES):
                rule_id = rule["id"]
                rule_name = rule["name"]
                
                if rule_id in result["rules"]:
                    rule_result = result["rules"][rule_id]
                    verdict = rule_result["verdict"]
                    reason = rule_result["reason"]
                    
                    # æ ¼å¼åŒ–æ˜¾ç¤º
                    if verdict == "ç¬¦åˆ":
                        icon = "ðŸ”´"  # é£Žé™©æ ‡è®°
                    elif verdict == "æ— æ³•éªŒè¯":
                        icon = "âš ï¸"  # æ— æ³•éªŒè¯æ ‡è®°
                    else:
                        icon = "âœ…"  # å®‰å…¨æ ‡è®°
                        
                    rules_output += f"{icon} **{rule_name}**\n{reason}\n\n"
            
            # æå–ç»¼åˆç»“è®º
            risk_level = result["risk_level"]
            risk_percentage = result["conclusion"]["risk_percentage"]
            explanation = result["conclusion"]["explanation"]
            
            # æ·»åŠ çŸ¥è¯†æˆªæ­¢æ—¥æœŸä¿¡æ¯
            knowledge_cutoff_info = f"\n\n**æ¨¡åž‹çŸ¥è¯†æˆªæ­¢æ—¥æœŸ**: {result.get('knowledge_cutoff_date', KNOWLEDGE_CUTOFF_DATE)}"
            
            # æ ¼å¼åŒ–ç»“è®º
            if risk_level == "é«˜é£Žé™©":
                level_icon = "âš ï¸"
            elif risk_level == "ä¸­ç­‰é£Žé™©":
                level_icon = "âš ï¸"
            else:
                level_icon = "âœ…"
                
            # å¤„ç†çŸ¥è¯†æˆªæ­¢æ—¥æœŸè­¦å‘Šä¿¡æ¯
            cutoff_warning = ""
            needs_search = False
            if "metadata" in result and result["metadata"].get("knowledge_cutoff_issue", False):
                cutoff_warning = "\n\nâš ï¸ **æ³¨æ„**: æ­¤æ–°é—»å¯èƒ½å‘ç”Ÿåœ¨æ¨¡åž‹çŸ¥è¯†æˆªæ­¢æ—¥æœŸä¹‹åŽ"
                needs_search = True
                
            conclusion = f"""## ç»¼åˆç»“è®º {level_icon}

**é£Žé™©è¯„ä¼°**: {risk_level} ({risk_percentage}%){cutoff_warning}

{explanation}{knowledge_cutoff_info}
"""

            # æ·»åŠ ç½‘ç»œéªŒè¯ç»“æžœï¼ˆå¦‚æžœæœ‰ï¼‰
            web_validation = ""
            if "web_validation" in result and result["web_validation"]:
                web_result = result["web_validation"]
                web_validation = f"## ç½‘ç»œæœç´¢éªŒè¯\n\n"
                
                # å¯ä¿¡æºæ•°é‡
                trusted_count = web_result["validation_results"]["trusted_sources_count"]
                web_validation += f"- å¯ä¿¡æ¥æºæ•°: {trusted_count}\n"
                
                # ä¸€è‡´æ€§å¾—åˆ†
                consistency = web_result["validation_results"]["consistency_score"]
                web_validation += f"- å†…å®¹ä¸€è‡´æ€§: {consistency}%\n\n"
                
                # æ¥æºåˆ—è¡¨
                sources = web_result["validation_results"].get("sources", [])
                if sources:
                    web_validation += "### å‚è€ƒæ¥æº\n\n"
                    for source in sources:
                        trust_marker = "âœ“" if source.get("is_trusted", False) else "âš ï¸"
                        domain = source.get("domain", "")
                        title = source.get("title", "")
                        url = source.get("url", "")
                        snippet = source.get("snippet", "")
                        
                        web_validation += f"**{trust_marker} {domain}**\n"
                        if title:
                            web_validation += f"**æ ‡é¢˜**: {title}\n"
                        if url:
                            web_validation += f"**é“¾æŽ¥**: [{url}]({url})\n"
                        if snippet:
                            web_validation += f"**æ‘˜è¦**: {snippet}\n"
                        web_validation += "\n"
                    
                    # å¦‚æžœæ²¡æœ‰æ˜¾ç¤ºæ¥æºï¼Œè®°å½•åˆ°æ—¥å¿—ä¸­è°ƒè¯•
                    if len(sources) == 0:
                        logger.warning("ç½‘ç»œæœç´¢ç»“æžœä¸­æ²¡æœ‰æ¥æºä¿¡æ¯")
                        logger.debug(f"å®Œæ•´çš„web_result: {json.dumps(web_result, ensure_ascii=False)}")
                else:
                    web_validation += "æœªæ‰¾åˆ°ç›¸å…³å‚è€ƒæ¥æº\n\n"
                    logger.warning("ç½‘ç»œæœç´¢ç»“æžœä¸­æ²¡æœ‰æ¥æºä¿¡æ¯")
                
                # ç»“è®º
                web_validation += f"### éªŒè¯ç»“è®º\n\n{web_result['explanation']}\n"
                
                # é£Žé™©è°ƒæ•´
                adjustment = web_result["risk_adjustment"]
                if adjustment != 0:
                    direction = "é™ä½Ž" if adjustment < 0 else "æé«˜"
                    web_validation += f"ï¼ˆé£Žé™©è¯„ä¼°{direction}äº†{abs(adjustment)}%ï¼‰"
            elif needs_search and not use_web_search:
                # å¦‚æžœå­˜åœ¨çŸ¥è¯†æˆªæ­¢æ—¥æœŸé—®é¢˜ä½†æœªå¯ç”¨ç½‘ç»œæœç´¢ï¼Œæä¾›è¯¦ç»†è§£é‡Š
                web_validation = f"""## çŸ¥è¯†æˆªæ­¢æ—¥æœŸé—®é¢˜è¯´æ˜Ž

æ­¤æ–°é—»æè¿°çš„äº‹ä»¶å¯èƒ½å‘ç”Ÿåœ¨æ¨¡åž‹çŸ¥è¯†æˆªæ­¢æ—¥æœŸï¼ˆ{KNOWLEDGE_CUTOFF_DATE}ï¼‰ä¹‹åŽï¼Œæ¨¡åž‹æ— æ³•éªŒè¯æ­¤ç±»äº‹ä»¶çš„çœŸå®žæ€§ã€‚

**å»ºè®®ä½¿ç”¨ç½‘ç»œæœç´¢éªŒè¯åŠŸèƒ½ä»¥èŽ·å–æ›´å‡†ç¡®çš„ç»“æžœã€‚**

è¯·ç‚¹å‡»"ä½¿ç”¨ç½‘ç»œæœç´¢éªŒè¯"æŒ‰é’®èŽ·å–æœ€æ–°ä¿¡æ¯ã€‚
"""

            return rules_output, conclusion, web_validation, needs_search
        except Exception as e:
            error_tb = traceback.format_exc()
            logger.error(f"å¤„ç†å¼‚å¸¸: {str(e)}\n{error_tb}")
            print(f"å¤„ç†å¼‚å¸¸: {str(e)}\n{error_tb}")
            
            # æž„å»ºç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            error_detail = f"å¤„ç†å‡ºé”™: {str(e)}"
            
            # æ·»åŠ ä¸€äº›å¸¸è§é”™è¯¯çš„ç‰¹å®šå¤„ç†
            if "APIè°ƒç”¨å¼‚å¸¸" in str(e):
                error_detail += "\n\nå¯èƒ½çš„åŽŸå› :\n1. APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ\n2. APIè°ƒç”¨é¢åº¦å·²ç”¨å®Œ\n3. ç½‘ç»œè¿žæŽ¥é—®é¢˜"
                error_detail += "\n\nè¯·æ£€æŸ¥æ‚¨çš„APIé…ç½®å¹¶å°è¯•é‡æ–°è¿è¡Œã€‚"
            elif "æ— æ³•è§£æžAPIå“åº”" in str(e):
                error_detail += "\n\nå¯èƒ½çš„åŽŸå› :\n1. APIå“åº”æ ¼å¼å‘ç”Ÿå˜åŒ–\n2. æ‚¨ä½¿ç”¨çš„æ¨¡åž‹ä¸Žé…ç½®ä¸ç¬¦"
                error_detail += "\n\nè¯·ç¡®è®¤æ‚¨ä½¿ç”¨çš„æ˜¯æ­£ç¡®çš„æ¨¡åž‹APIä¸”é…ç½®æ­£ç¡®ã€‚"
            
            return error_detail, "æ— æ³•å®Œæˆåˆ†æž", "", False

    # å•ç‹¬å®žçŽ°ç½‘ç»œæœç´¢éªŒè¯å‡½æ•°ï¼Œé¿å…é‡å¤è°ƒç”¨æ¨¡åž‹
    def run_web_search(title, content, url=""):
        """åªæ‰§è¡Œç½‘ç»œæœç´¢éªŒè¯ï¼Œä¸è°ƒç”¨æ¨¡åž‹
        
        Args:
            title: æ–°é—»æ ‡é¢˜
            content: æ–°é—»å†…å®¹
            url: æ–°é—»æ¥æºURL
            
        Returns:
            ä¸Ždetect_newsç›¸åŒçš„è¾“å‡º
        """
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„åˆ†æžç»“æžœ
        if last_analysis_result is None:
            return "è¯·å…ˆç‚¹å‡»'å¼€å§‹æ£€æµ‹'æŒ‰é’®åˆ†æžæ–°é—»å†…å®¹", "", "éœ€è¦å…ˆè¿›è¡Œæ¨¡åž‹åˆ†æžæ‰èƒ½ä½¿ç”¨ç½‘ç»œæœç´¢éªŒè¯", False
        
        # ä½¿ç”¨detect_newså‡½æ•°ï¼Œä½†è®¾ç½®use_web_search=True
        return detect_news(title, content, url, use_web_search=True)

    # è®¾è®¡UIç•Œé¢
    with gr.Blocks(title=WEB_CONFIG["TITLE"], theme=WEB_CONFIG["THEME"]) as app:
        gr.Markdown(f"""
        # {WEB_CONFIG["TITLE"]}
        
        ðŸ“… æ¨¡åž‹çŸ¥è¯†æˆªæ­¢æ—¥æœŸ: {KNOWLEDGE_CUTOFF_DATE}
        """)

        if enable_web_search:
            gr.Markdown("âœ“ ç½‘ç»œæœç´¢éªŒè¯åŠŸèƒ½å¯ç”¨ï¼Œå¯ä»¥éªŒè¯æ—¶æ•ˆæ€§æ–°é—»")
        else:
            gr.Markdown("âš ï¸ ç½‘ç»œæœç´¢åŠŸèƒ½å¯ç”¨ï¼Œä½†é»˜è®¤ä¸å¯ç”¨ã€‚æ‚¨å¯ä»¥åœ¨åˆ†æžåŽï¼Œå¯¹äºŽ{KNOWLEDGE_CUTOFF_DATE}ä¹‹åŽçš„æ–°é—»é€‰æ‹©ä½¿ç”¨ç½‘ç»œæœç´¢éªŒè¯")

        with gr.Row():
            with gr.Column():
                title_input = gr.Textbox(
                    label="æ–°é—»æ ‡é¢˜",
                    placeholder="è¯·è¾“å…¥æ–°é—»æ ‡é¢˜"
                )
                
                content_input = gr.Textbox(
                    label="æ–°é—»å†…å®¹",
                    placeholder="è¯·è¾“å…¥æ–°é—»æ­£æ–‡å†…å®¹",
                    lines=10
                )
                
                url_input = gr.Textbox(
                    label="æ–°é—»æ¥æºURL (å¯é€‰)",
                    placeholder="ä¾‹å¦‚: https://example.com/news"
                )
                
                with gr.Row():
                    detect_button = gr.Button("å¼€å§‹æ£€æµ‹", variant="primary")
                    web_search_button = gr.Button("ä½¿ç”¨ç½‘ç»œæœç´¢éªŒè¯", variant="secondary")
                
            with gr.Column():
                rules_output = gr.Markdown(label="è§„åˆ™åˆ†æžç»“æžœ")
                conclusion_output = gr.Markdown(label="ç»¼åˆç»“è®º")
                web_validation_output = gr.Markdown(label="ç½‘ç»œéªŒè¯ç»“æžœ")
                needs_search = gr.Checkbox(visible=False)

        # æ·»åŠ ç¤ºä¾‹
        gr.Examples(
            [
                ["åœ°éœ‡æ•‘æ´ç‹—åœ¨åºŸå¢Ÿä¸­æ•‘å‡º15äºº", "æ˜¨æ—¥ï¼Œä¸€åªæœæ•‘çŠ¬åœ¨åœ°éœ‡åºŸå¢Ÿä¸­æˆåŠŸæ‰¾åˆ°å¹¶æ•‘å‡ºäº†15åè¢«å›°äººå‘˜ï¼Œåˆ›ä¸‹äº†æœæ•‘çŠ¬æ•‘æ´è®°å½•...", ""],
                ["ç§‘å­¦å®¶å‘çŽ°å–å’–å•¡èƒ½é¢„é˜²ç™Œç—‡", "è¿‘æ—¥ï¼Œä¸€é¡¹æ–°ç ”ç©¶è¡¨æ˜Žï¼Œæ¯å¤©å–3æ¯å’–å•¡å¯ä»¥å°†ç™Œç—‡é£Žé™©é™ä½Ž80%ï¼Œè¿™ä¸€å‘çŽ°éœ‡æƒŠäº†åŒ»å­¦ç•Œ...", "health.example.org"],
                ["2024å¹´ä¸–ç•Œç»æµŽè®ºå›åœ¨è¾¾æ²ƒæ–¯å¼€å¹•", "2024å¹´1æœˆ15æ—¥ï¼Œç¬¬54å±Šä¸–ç•Œç»æµŽè®ºå›å¹´ä¼šåœ¨ç‘žå£«è¾¾æ²ƒæ–¯å¼€å¹•ã€‚æ¥è‡ª130å¤šä¸ªå›½å®¶çš„2800å¤šåé¢†å¯¼äººã€ä¼ä¸šå®¶å’Œä¸“å®¶é½èšä¸€å ‚ï¼Œå…±åŒè®¨è®ºå…¨çƒç»æµŽå½¢åŠ¿å’Œæœªæ¥å‘å±•æ–¹å‘...", ""],
            ],
            [title_input, content_input, url_input],
            "ç‚¹å‡»åŠ è½½ç¤ºä¾‹"
        )
        
        # è®¾ç½®ç‚¹å‡»äº‹ä»¶
        detect_button.click(
            fn=detect_news,
            inputs=[title_input, content_input, url_input, gr.Checkbox(value=False)],
            outputs=[rules_output, conclusion_output, web_validation_output, needs_search]
        )

        web_search_button.click(
            fn=run_web_search,  # ä½¿ç”¨å•ç‹¬çš„å‡½æ•°å¤„ç†ç½‘ç»œæœç´¢
            inputs=[title_input, content_input, url_input],
            outputs=[rules_output, conclusion_output, web_validation_output, needs_search]
        )

    return app


def launch_app(enable_web_search=False, port=None):
    """å¯åŠ¨Webåº”ç”¨
    
    Args:
        enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢éªŒè¯
        port: æœåŠ¡ç«¯å£ï¼Œå¦‚æžœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„ç«¯å£
    """
    app = create_app(enable_web_search)
    # å¦‚æžœæä¾›äº†è‡ªå®šä¹‰ç«¯å£ï¼Œåˆ™ä½¿ç”¨å®ƒï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤ç«¯å£
    server_port = port if port is not None else WEB_CONFIG["PORT"]
    
    # å°è¯•å¯åŠ¨åº”ç”¨ï¼Œå¦‚æžœç«¯å£è¢«å ç”¨åˆ™å°è¯•å…¶ä»–ç«¯å£
    try:
        app.launch(server_port=server_port)
    except OSError as e:
        if "address already in use" in str(e).lower():
            print(f"ç«¯å£ {server_port} å·²è¢«å ç”¨ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–ç«¯å£...")
            for alt_port in range(7865, 7880):
                if alt_port == server_port:
                    continue
                try:
                    print(f"å°è¯•ç«¯å£ {alt_port}...")
                    app.launch(server_port=alt_port)
                    break
                except OSError:
                    continue
            else:
                print("æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®šä¸€ä¸ªæœªè¢«å ç”¨çš„ç«¯å£")
                print("ä¾‹å¦‚: python main.py --web --port 8000")
        else:
            raise

if __name__ == "__main__":
    launch_app()

